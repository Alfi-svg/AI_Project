# AI_Project
This project presents a fully offline AI-based video analysis system developed for an AI hackathon.
# ğŸ“˜ README.md

## Offline AI-Based Video Analysis System

**(Phase-I & Phase-II Combined)**

---

## ğŸ“Œ Project Overview

This project presents a **fully offline, AI-based video analysis system** developed for an AI hackathon.
The system is capable of analyzing video content to:

* Extract speech and generate text (Bangla & English)
* Perform Bangladesh-context sentiment analysis
* Generate explainable AI outputs
* Analyze presenter motion and visual behavior (non-verbal cues)

The entire system operates **without any external API, cloud service, or internet dependency**, strictly following all competition constraints.

---

## ğŸ¯ Project Objectives

### Phase-I Objectives:

* Convert video speech into text using a trained ASR model
* Analyze generated text for sentiment (Positive, Negative, Neutral)
* Provide explainable AI-based reasoning for sentiment decisions

### Phase-II Objectives:

* Analyze presenter posture, gestures, eye-contact approximation, and movement
* Extract time-based, explainable visual and motion features
* Generate structured datasets and statistical summaries

---

## ğŸ§  AI Models Used

### Automatic Speech Recognition (ASR)

* Framework: TensorFlow / Keras
* Model Type: End-to-End Speech-to-Text
* Loss Function: CTC (Connectionist Temporal Classification)
* Transcription Level: Character-level
* Languages Supported: Bangla and English
* Model Format: TensorFlow SavedModel

Model loading is performed using:

```python
tf.keras.models.load_model()
```

---

## ğŸ—‚ï¸ Project Structure

```text
AI_VIDEO_ANALYSIS_PROJECT/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ asr/
â”‚   â”œâ”€â”€ sentiment/
â”‚   â”œâ”€â”€ vision/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ BengaliDictionary_93.json
â”‚   â””â”€â”€ model/
â”‚       â”œâ”€â”€ saved_model.pb
â”‚       â””â”€â”€ variables/
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ documentation/
    â”œâ”€â”€ Phase_I_Documentation.pdf
    â””â”€â”€ Phase_II_Documentation.pdf
```

---

## âš™ï¸ System Workflow

### Phase-I Workflow:

1. User provides video input (file or offline link)
2. Audio is extracted from the video
3. Speech is converted to text using the ASR model
4. Text is analyzed for sentiment (Bangladesh context)
5. Explainable AI output is generated

### Phase-II Workflow:

1. Video is segmented into frames or time intervals
2. Visual features are extracted from each segment
3. Motion and behavior signals are generated
4. Timeline datasets and statistical summaries are produced

---

## ğŸ“Š Extracted Features (Phase-II)

* **Posture Openness** â€“ Open vs closed body posture
* **Hand Gesture Activity** â€“ Frequency and intensity of gestures
* **Eye-Contact Approximation** â€“ Camera-facing estimation (approximate)
* **Overall Movement & Pacing** â€“ Stability and variability of movement

All features are **time-based, explainable, and interpretable**.

---

## ğŸ“ Output Deliverables

* Speech transcription (JSON)
* Sentiment classification with explanation
* Movement timeline dataset (CSV / JSON)
* Statistical summaries of motion patterns
* Professional documentation (PDF)

---

## ğŸ”’ Offline Compliance

This project strictly follows offline operation rules:

* âŒ No API calls
* âŒ No cloud-based services
* âŒ No internet dependency
* âŒ No external hosted models
* âœ… All processing is local

---

## ğŸ§ª How to Run the Project

### 1. Environment Setup

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Run Backend

```bash
cd backend
export ALLOW_NET_DOWNLOAD=1
uvicorn main:app --reload
```

### 3. Access API

```
http://127.0.0.1:8000/docs
```

### 4. Test UI

Open:

```text
ui/index.html
```

---

## ğŸ“Œ Limitations

* Eye-contact estimation is approximate
* Performance depends on video quality
* Camera angle and lighting may affect results
* Partial occlusion can impact feature extraction

All limitations are transparently documented.

---

## ğŸ Conclusion

This project demonstrates a **robust, fully offline AI system** capable of analyzing both verbal and non-verbal aspects of video content.
By combining explainable speech analysis and motion-based visual features, the system provides meaningful insights while maintaining transparency, interpretability, and strict compliance with competition rules.

---

## ğŸ‘¨â€ğŸ’» Submission Notes

* The entire project is submitted as a ZIP file
* Screen recording demonstrates system functionality
* All documentation is provided in PDF format
* Google Drive sharing is enabled for evaluation

---

âœ… **This README is FINAL SUBMISSION READY**
Just save it as `README.md` in your project root.

---

